{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cell 1**: 套件導入與 Tokenizer 載入 (共 3 個 code cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, RobertaTokenizer, RobertaModel, GPT2Tokenizer, GPT2Model\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#  You can install and import any other libraries if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Chinese punctuations will be tokenized as [UNK], so we replace them with English ones\n",
    "# 全形符號換半形符號，否則模型無法辨識(不在辭庫)\n",
    "token_replacement = [\n",
    "    [\"：\" , \":\"],\n",
    "    [\"，\" , \",\"],\n",
    "    [\"“\" , \"\\\"\"],\n",
    "    [\"”\" , \"\\\"\"],\n",
    "    [\"？\" , \"?\"],\n",
    "    [\"……\" , \"...\"],\n",
    "    [\"！\" , \"!\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizers for different models\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"./cache/\")\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", cache_dir=\"./cache/\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", cache_dir=\"./cache/\")\n",
    "# GPT2 doesn't have a padding token by default, so we set it\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cell 5**: 資料集定義與載入 (共 2 個 code cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SICK dataset (train split)...\n",
      "Loaded 4439 samples from SICK TRAIN set\n",
      "\n",
      "First sample:\n",
      "Premise: A group of kids is playing in a yard and an old man is standing in the background...\n",
      "Hypothesis: A group of boys in a yard is playing and a man is standing in the background...\n",
      "Relatedness: 4.5\n",
      "Entailment: 1\n"
     ]
    }
   ],
   "source": [
    "# Input: Two sentences (\"premise\", \"hypothesis\") \n",
    "# Output: 1. Similarity score 2. Entailment relation\n",
    "import csv\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SemevalDataset(Dataset):\n",
    "    def __init__(self, split=\"train\") -> None:\n",
    "        super().__init__()\n",
    "        print(f\"Loading SICK dataset ({split} split)...\")\n",
    "        \n",
    "        # Read SICK.txt file\n",
    "        sick_file = \"./SICK.txt\"\n",
    "        self.data = []\n",
    "        \n",
    "        # Map split names to SICK dataset splits\n",
    "        split_map = {\n",
    "            \"train\": \"TRAIN\",\n",
    "            \"validation\": \"TRIAL\",  # Use TRIAL as validation\n",
    "            \"test\": \"TEST\"\n",
    "        }\n",
    "        target_split = split_map.get(split, \"TRAIN\")\n",
    "        \n",
    "        # Read TSV file\n",
    "        with open(sick_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            for row in reader:\n",
    "                # Filter by split\n",
    "                if row['SemEval_set'] == target_split:\n",
    "                    # Get entailment label\n",
    "                    entailment_str = row['entailment_label']\n",
    "                    \n",
    "                    self.data.append({\n",
    "                        \"sentence_A\": row['sentence_A'],\n",
    "                        \"sentence_B\": row['sentence_B'],\n",
    "                        \"relatedness_score\": float(row['relatedness_score']),\n",
    "                        \"entailment_judgment\": entailment_str\n",
    "                    })\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} samples from SICK {target_split} set\")\n",
    "    \n",
    "    # Entailment label mapping\n",
    "    def _map_label(self, label):\n",
    "        \"\"\"Map string labels to integers\"\"\"\n",
    "        if isinstance(label, int):\n",
    "            return label\n",
    "        label_map = {\n",
    "            \"ENTAILMENT\": 0,       # Entailment: B can be inferred from A\n",
    "            \"NEUTRAL\": 1,          # Neutral: A and B have no clear logical relationship\n",
    "            \"CONTRADICTION\": 2     # Contradiction: A and B conflict with each other\n",
    "        }\n",
    "        return label_map.get(label.upper() if isinstance(label, str) else \"NEUTRAL\", 1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        d = self.data[index]\n",
    "        \n",
    "        # Extract fields from SICK format\n",
    "        sentence_A = d.get(\"sentence_A\", \"\")\n",
    "        sentence_B = d.get(\"sentence_B\", \"\")\n",
    "        \n",
    "        # Relatedness score (1-5 in SICK)\n",
    "        relatedness = float(d.get(\"relatedness_score\", 3.0))\n",
    "        \n",
    "        # Entailment label\n",
    "        entailment_raw = d.get(\"entailment_judgment\", \"NEUTRAL\")\n",
    "        entailment = self._map_label(entailment_raw)\n",
    "        \n",
    "        result = {\n",
    "            \"premise\": str(sentence_A),\n",
    "            \"hypothesis\": str(sentence_B),\n",
    "            \"relatedness_score\": relatedness,\n",
    "            \"entailment_judgment\": entailment\n",
    "        }\n",
    "        \n",
    "        # Replace full-width punctuation with half-width, otherwise model cannot recognize (not in vocabulary)\n",
    "        for k in [\"premise\", \"hypothesis\"]:\n",
    "            for tok in token_replacement:\n",
    "                result[k] = result[k].replace(tok[0], tok[1])\n",
    "        return result\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "# Test loading\n",
    "dataset_train = SemevalDataset(split=\"train\")\n",
    "print(f\"\\nFirst sample:\")\n",
    "sample = dataset_train[0]\n",
    "print(f\"Premise: {sample['premise'][:100]}...\")\n",
    "print(f\"Hypothesis: {sample['hypothesis'][:100]}...\")\n",
    "print(f\"Relatedness: {sample['relatedness_score']}\")\n",
    "print(f\"Entailment: {sample['entailment_judgment']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SICK dataset (train split)...\n",
      "Loaded 4439 samples from SICK TRAIN set\n",
      "Loading SICK dataset (validation split)...\n",
      "Loaded 495 samples from SICK TRIAL set\n",
      "Loading SICK dataset (test split)...\n",
      "Loaded 4906 samples from SICK TEST set\n",
      "訓練集大小: 4439\n",
      "驗證集大小: 495\n",
      "測試集大小: 4906\n"
     ]
    }
   ],
   "source": [
    "train_set = SemevalDataset(split='train')\n",
    "val_set = SemevalDataset(split='validation')\n",
    "test_set = SemevalDataset(split='test')\n",
    "\n",
    "print(f\"訓練集大小: {len(train_set)}\")\n",
    "print(f\"驗證集大小: {len(val_set)}\")\n",
    "print(f\"測試集大小: {len(test_set)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cell 8**: 超參數設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "# You can modify these values if needed\n",
    "lr = 3e-5\n",
    "epochs = 10\n",
    "train_batch_size = 8\n",
    "validation_batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cell 10**: BERT DataLoader 與 Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SICK dataset (train split)...\n",
      "Loaded 4439 samples from SICK TRAIN set\n",
      "Loading SICK dataset (validation split)...\n",
      "Loaded 495 samples from SICK TRIAL set\n",
      "Loading SICK dataset (test split)...\n",
      "Loaded 4906 samples from SICK TEST set\n"
     ]
    }
   ],
   "source": [
    "# TODO1: Create batched data for DataLoader\n",
    "# `collate_fn` is a function that defines how the data batch should be packed.\n",
    "# This function will be called in the DataLoader to pack the data batch.\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # TODO1-1: Implement the collate_fn function\n",
    "    # Write your code here\n",
    "    # The input parameter is a data batch (tuple), and this function packs it into tensors.\n",
    "    # Use tokenizer to pack tokenize and pack the data and its corresponding labels.\n",
    "    # Return the data batch and labels for each sub-task.\n",
    "    \n",
    "    # Write your code here\n",
    "    premises = [item[\"premise\"] for item in batch]\n",
    "    hypotheses = [item[\"hypothesis\"] for item in batch]\n",
    "    relatedness_scores = torch.tensor([item[\"relatedness_score\"] for item in batch], dtype=torch.float)\n",
    "    entailment_labels = torch.tensor([item[\"entailment_judgment\"] for item in batch], dtype=torch.long)\n",
    "    \n",
    "    # Tokenize the premise and hypothesis pairs\n",
    "    encodings = tokenizer(\n",
    "        premises, \n",
    "        hypotheses, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=128, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return encodings, relatedness_scores, entailment_labels\n",
    "\n",
    "# TODO1-2: Define your DataLoader\n",
    "dl_train = DataLoader(SemevalDataset(split=\"train\"), batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn) # Write your code here\n",
    "dl_validation = DataLoader(SemevalDataset(split=\"validation\"), batch_size=validation_batch_size, shuffle=False, collate_fn=collate_fn) # Write your code here\n",
    "dl_test = DataLoader(SemevalDataset(split=\"test\"), batch_size=validation_batch_size, shuffle=False, collate_fn=collate_fn) # Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cell 12**: RoBERTa 與 GPT-2 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SICK dataset (train split)...\n",
      "Loaded 4439 samples from SICK TRAIN set\n",
      "Loading SICK dataset (validation split)...\n",
      "Loaded 495 samples from SICK TRIAL set\n",
      "Loading SICK dataset (test split)...\n",
      "Loaded 4906 samples from SICK TEST set\n",
      "Loading SICK dataset (train split)...\n",
      "Loaded 4439 samples from SICK TRAIN set\n",
      "Loading SICK dataset (validation split)...\n",
      "Loaded 495 samples from SICK TRIAL set\n",
      "Loading SICK dataset (test split)...\n",
      "Loaded 4906 samples from SICK TEST set\n",
      "Loaded 495 samples from SICK TRIAL set\n",
      "Loading SICK dataset (test split)...\n",
      "Loaded 4906 samples from SICK TEST set\n",
      "Loading SICK dataset (train split)...\n",
      "Loaded 4439 samples from SICK TRAIN set\n",
      "Loading SICK dataset (validation split)...\n",
      "Loaded 495 samples from SICK TRIAL set\n",
      "Loading SICK dataset (test split)...\n",
      "Loaded 4906 samples from SICK TEST set\n"
     ]
    }
   ],
   "source": [
    "# Collate functions for different tokenizers\n",
    "def collate_fn_roberta(batch):\n",
    "    premises = [item[\"premise\"] for item in batch]\n",
    "    hypotheses = [item[\"hypothesis\"] for item in batch]\n",
    "    relatedness_scores = torch.tensor([item[\"relatedness_score\"] for item in batch], dtype=torch.float)\n",
    "    entailment_labels = torch.tensor([item[\"entailment_judgment\"] for item in batch], dtype=torch.long)\n",
    "    \n",
    "    encodings = roberta_tokenizer(\n",
    "        premises, \n",
    "        hypotheses, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=128, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return encodings, relatedness_scores, entailment_labels\n",
    "\n",
    "def collate_fn_gpt2(batch):\n",
    "    premises = [item[\"premise\"] for item in batch]\n",
    "    hypotheses = [item[\"hypothesis\"] for item in batch]\n",
    "    relatedness_scores = torch.tensor([item[\"relatedness_score\"] for item in batch], dtype=torch.float)\n",
    "    entailment_labels = torch.tensor([item[\"entailment_judgment\"] for item in batch], dtype=torch.long)\n",
    "    \n",
    "    # For GPT2, we concatenate premise and hypothesis with a separator\n",
    "    texts = [f\"{p} {gpt2_tokenizer.eos_token} {h}\" for p, h in zip(premises, hypotheses)]\n",
    "    encodings = gpt2_tokenizer(\n",
    "        texts,\n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=128, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return encodings, relatedness_scores, entailment_labels\n",
    "\n",
    "# DataLoaders for RoBERTa\n",
    "dl_train_roberta = DataLoader(SemevalDataset(split=\"train\"), batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn_roberta)\n",
    "dl_validation_roberta = DataLoader(SemevalDataset(split=\"validation\"), batch_size=validation_batch_size, shuffle=False, collate_fn=collate_fn_roberta)\n",
    "dl_test_roberta = DataLoader(SemevalDataset(split=\"test\"), batch_size=validation_batch_size, shuffle=False, collate_fn=collate_fn_roberta)\n",
    "\n",
    "# DataLoaders for GPT2\n",
    "dl_train_gpt2 = DataLoader(SemevalDataset(split=\"train\"), batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn_gpt2)\n",
    "dl_validation_gpt2 = DataLoader(SemevalDataset(split=\"validation\"), batch_size=validation_batch_size, shuffle=False, collate_fn=collate_fn_gpt2)\n",
    "dl_test_gpt2 = DataLoader(SemevalDataset(split=\"test\"), batch_size=validation_batch_size, shuffle=False, collate_fn=collate_fn_gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cell 14**: BERT 多任務模型定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO2: Construct your model\n",
    "class MultiLabelModel(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Write your code here\n",
    "        # Define what modules you will use in the model\n",
    "        # Please use \"google-bert/bert-base-uncased\" model (https://huggingface.co/google-bert/bert-base-uncased)\n",
    "        # Besides the base model, you may design additional architectures by incorporating linear layers, activation functions, or other neural components.\n",
    "        # Remark: The use of any additional pretrained language models is not permitted.\n",
    "        \n",
    "        # Write your code here\n",
    "        self.bert = BertModel.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"./cache/\")\n",
    "        hidden_size = self.bert.config.hidden_size  # 768 for bert-base\n",
    "        \n",
    "        # Regression head for relatedness score prediction\n",
    "        self.relatedness_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(256, 1)  # Output a single score\n",
    "        )\n",
    "        \n",
    "        # Classification head for entailment judgment (3 classes: ENTAILMENT, CONTRADICTION, NEUTRAL)\n",
    "        self.entailment_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(256, 3)  # Output 3 classes\n",
    "        )\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        # Write your code here\n",
    "        # Forward pass\n",
    "        \n",
    "        # Write your code here\n",
    "        # Get BERT outputs\n",
    "        outputs = self.bert(**kwargs)\n",
    "        # Use [CLS] token representation for classification\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "        \n",
    "        # Get predictions for both tasks\n",
    "        relatedness_score = self.relatedness_head(cls_output).squeeze(-1)  # Shape: (batch_size,)\n",
    "        entailment_logits = self.entailment_head(cls_output)  # Shape: (batch_size, 3)\n",
    "        \n",
    "        return relatedness_score, entailment_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cell 16**: RoBERTa、GPT-2 與單任務 BERT 模型定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoBERTa Multi-output Model\n",
    "class MultiLabelModelRoBERTa(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\", cache_dir=\"./cache/\")\n",
    "        hidden_size = self.roberta.config.hidden_size  # 768 for roberta-base\n",
    "        \n",
    "        # Regression head for relatedness score prediction\n",
    "        self.relatedness_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        # Classification head for entailment judgment\n",
    "        self.entailment_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(256, 3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        outputs = self.roberta(**kwargs)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        relatedness_score = self.relatedness_head(cls_output).squeeze(-1)\n",
    "        entailment_logits = self.entailment_head(cls_output)\n",
    "        \n",
    "        return relatedness_score, entailment_logits\n",
    "\n",
    "# GPT-2 Multi-output Model\n",
    "class MultiLabelModelGPT2(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.gpt2 = GPT2Model.from_pretrained(\"gpt2\", cache_dir=\"./cache/\")\n",
    "        hidden_size = self.gpt2.config.hidden_size  # 768 for gpt2\n",
    "        \n",
    "        # Regression head for relatedness score prediction\n",
    "        self.relatedness_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        # Classification head for entailment judgment\n",
    "        self.entailment_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(256, 3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        outputs = self.gpt2(**kwargs)\n",
    "        # GPT2 doesn't have a [CLS] token, so we use the last token's representation\n",
    "        last_token_output = outputs.last_hidden_state[:, -1, :]\n",
    "        \n",
    "        relatedness_score = self.relatedness_head(last_token_output).squeeze(-1)\n",
    "        entailment_logits = self.entailment_head(last_token_output)\n",
    "        \n",
    "        return relatedness_score, entailment_logits\n",
    "\n",
    "# Single-task BERT Models (for comparison with multi-output learning)\n",
    "class SingleTaskRelatednessModel(torch.nn.Module):\n",
    "    \"\"\"BERT model for relatedness score prediction only\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.bert = BertModel.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"./cache/\")\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        self.relatedness_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        outputs = self.bert(**kwargs)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        relatedness_score = self.relatedness_head(cls_output).squeeze(-1)\n",
    "        return relatedness_score\n",
    "\n",
    "class SingleTaskEntailmentModel(torch.nn.Module):\n",
    "    \"\"\"BERT model for entailment classification only\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.bert = BertModel.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"./cache/\")\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        self.entailment_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(256, 3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        outputs = self.bert(**kwargs)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        entailment_logits = self.entailment_head(cls_output)\n",
    "        return entailment_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cell 18**: 優化器與損失函數定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO3: Define your optimizer and loss function\n",
    "\n",
    "model = MultiLabelModel().to(device)\n",
    "# TODO3-1: Define your Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=lr) # Write your code here\n",
    "\n",
    "# TODO3-2: Define your loss functions (you should have two)\n",
    "# Write your code here\n",
    "# Loss function for regression task (relatedness score)\n",
    "relatedness_loss_fn = torch.nn.MSELoss()\n",
    "# Loss function for classification task (entailment judgment)\n",
    "entailment_loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cell 20**: BERT 模型訓練與驗證 (共 2 個 code cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training BERT-base Multi-output Model\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create directory for saving models\n",
    "import os\n",
    "os.makedirs('./saved_models', exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Training BERT-base Multi-output Model\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[BERT] Training epoch [1/10]: 100%|██████████| 555/555 [00:23<00:00, 24.08it/s, loss=0.387]\n",
      "[BERT] Validation epoch [1/10]: 100%|██████████| 62/62 [00:00<00:00, 159.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BERT] Epoch 1/10 - Pearson Correlation: 0.8616, Accuracy: 0.8505\n",
      "[BERT] Best model saved with score: 1.7121\n",
      "\n",
      "[BERT] Best model saved with score: 1.7121\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[BERT] Training epoch [2/10]: 100%|██████████| 555/555 [00:26<00:00, 21.12it/s, loss=0.363]\n",
      "[BERT] Training epoch [2/10]: 100%|██████████| 555/555 [00:26<00:00, 21.12it/s, loss=0.363]\n",
      "[BERT] Validation epoch [2/10]: 100%|██████████| 62/62 [00:00<00:00, 156.84it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BERT] Epoch 2/10 - Pearson Correlation: 0.8702, Accuracy: 0.8545\n",
      "[BERT] Best model saved with score: 1.7247\n",
      "\n",
      "[BERT] Best model saved with score: 1.7247\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[BERT] Training epoch [3/10]: 100%|██████████| 555/555 [00:24<00:00, 22.87it/s, loss=0.543] \n",
      "[BERT] Training epoch [3/10]: 100%|██████████| 555/555 [00:24<00:00, 22.87it/s, loss=0.543]\n",
      "[BERT] Validation epoch [3/10]: 100%|██████████| 62/62 [00:00<00:00, 156.00it/s]\n",
      "[BERT] Validation epoch [3/10]: 100%|██████████| 62/62 [00:00<00:00, 156.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BERT] Epoch 3/10 - Pearson Correlation: 0.8698, Accuracy: 0.8626\n",
      "[BERT] Best model saved with score: 1.7324\n",
      "\n",
      "[BERT] Best model saved with score: 1.7324\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[BERT] Training epoch [4/10]: 100%|██████████| 555/555 [00:27<00:00, 20.04it/s, loss=0.414] \n",
      "[BERT] Training epoch [4/10]: 100%|██████████| 555/555 [00:27<00:00, 20.04it/s, loss=0.414]\n",
      "[BERT] Validation epoch [4/10]: 100%|██████████| 62/62 [00:00<00:00, 149.52it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BERT] Epoch 4/10 - Pearson Correlation: 0.8716, Accuracy: 0.8343\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[BERT] Training epoch [5/10]: 100%|██████████| 555/555 [00:27<00:00, 20.07it/s, loss=0.148] \n",
      "[BERT] Training epoch [5/10]: 100%|██████████| 555/555 [00:27<00:00, 20.07it/s, loss=0.148]\n",
      "[BERT] Validation epoch [5/10]: 100%|██████████| 62/62 [00:00<00:00, 148.87it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BERT] Epoch 5/10 - Pearson Correlation: 0.8889, Accuracy: 0.8747\n",
      "[BERT] Best model saved with score: 1.7636\n",
      "\n",
      "[BERT] Best model saved with score: 1.7636\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[BERT] Training epoch [6/10]: 100%|██████████| 555/555 [00:25<00:00, 21.40it/s, loss=0.44]  \n",
      "[BERT] Training epoch [6/10]: 100%|██████████| 555/555 [00:25<00:00, 21.40it/s, loss=0.44] \n",
      "[BERT] Validation epoch [6/10]: 100%|██████████| 62/62 [00:00<00:00, 156.39it/s]\n",
      "[BERT] Validation epoch [6/10]: 100%|██████████| 62/62 [00:00<00:00, 156.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BERT] Epoch 6/10 - Pearson Correlation: 0.8700, Accuracy: 0.8566\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[BERT] Training epoch [7/10]: 100%|██████████| 555/555 [00:26<00:00, 20.68it/s, loss=0.143] \n",
      "[BERT] Training epoch [7/10]: 100%|██████████| 555/555 [00:26<00:00, 20.68it/s, loss=0.143]\n",
      "[BERT] Validation epoch [7/10]: 100%|██████████| 62/62 [00:00<00:00, 153.69it/s]\n",
      "[BERT] Validation epoch [7/10]: 100%|██████████| 62/62 [00:00<00:00, 153.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BERT] Epoch 7/10 - Pearson Correlation: 0.8851, Accuracy: 0.8727\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[BERT] Training epoch [8/10]: 100%|██████████| 555/555 [00:24<00:00, 22.76it/s, loss=0.109] \n",
      "[BERT] Training epoch [8/10]: 100%|██████████| 555/555 [00:24<00:00, 22.76it/s, loss=0.109]\n",
      "[BERT] Validation epoch [8/10]: 100%|██████████| 62/62 [00:00<00:00, 157.26it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BERT] Epoch 8/10 - Pearson Correlation: 0.8555, Accuracy: 0.8424\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[BERT] Training epoch [9/10]: 100%|██████████| 555/555 [00:27<00:00, 20.24it/s, loss=0.118] \n",
      "[BERT] Training epoch [9/10]: 100%|██████████| 555/555 [00:27<00:00, 20.24it/s, loss=0.118]\n",
      "[BERT] Validation epoch [9/10]: 100%|██████████| 62/62 [00:00<00:00, 149.75it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BERT] Epoch 9/10 - Pearson Correlation: 0.8745, Accuracy: 0.8566\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[BERT] Training epoch [10/10]: 100%|██████████| 555/555 [00:25<00:00, 21.64it/s, loss=0.0711]\n",
      "[BERT] Training epoch [10/10]: 100%|██████████| 555/555 [00:25<00:00, 21.64it/s, loss=0.0711]\n",
      "[BERT] Validation epoch [10/10]: 100%|██████████| 62/62 [00:00<00:00, 156.97it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BERT] Epoch 10/10 - Pearson Correlation: 0.8756, Accuracy: 0.8606\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_score = 0.0\n",
    "for ep in range(epochs):\n",
    "    pbar = tqdm(dl_train)\n",
    "    pbar.set_description(f\"[BERT] Training epoch [{ep+1}/{epochs}]\")\n",
    "    model.train()\n",
    "    # TODO4: Write the training loop\n",
    "    # Write your code here\n",
    "    # train your model\n",
    "    # clear gradient\n",
    "    # forward pass\n",
    "    # compute loss\n",
    "    # back-propagation\n",
    "    # model optimization\n",
    "    \n",
    "    # Write your code here\n",
    "    for batch in pbar:\n",
    "        encodings, relatedness_scores, entailment_labels = batch\n",
    "        encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "        relatedness_scores = relatedness_scores.to(device)\n",
    "        entailment_labels = entailment_labels.to(device)\n",
    "        \n",
    "        # Clear gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        pred_relatedness, pred_entailment = model(**encodings)\n",
    "        \n",
    "        # Compute loss for both tasks\n",
    "        loss_relatedness = relatedness_loss_fn(pred_relatedness, relatedness_scores)\n",
    "        loss_entailment = entailment_loss_fn(pred_entailment, entailment_labels)\n",
    "        # Combine losses (you can adjust weights if needed)\n",
    "        loss = loss_relatedness + loss_entailment\n",
    "        \n",
    "        # Back-propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Model optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    pbar = tqdm(dl_validation)\n",
    "    pbar.set_description(f\"[BERT] Validation epoch [{ep+1}/{epochs}]\")\n",
    "    model.eval()\n",
    "    # TODO5: Write the evaluation loop\n",
    "    # Write your code here\n",
    "    # Evaluate your model\n",
    "    # Output all the evaluation scores (PearsonCorr, Accuracy)\n",
    "    \n",
    "    # Write your code here\n",
    "    all_pred_relatedness = []\n",
    "    all_true_relatedness = []\n",
    "    all_pred_entailment = []\n",
    "    all_true_entailment = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            encodings, relatedness_scores, entailment_labels = batch\n",
    "            encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_relatedness, pred_entailment = model(**encodings)\n",
    "            \n",
    "            # Collect predictions\n",
    "            all_pred_relatedness.extend(pred_relatedness.cpu().numpy().tolist())\n",
    "            all_true_relatedness.extend(relatedness_scores.numpy().tolist())\n",
    "            all_pred_entailment.extend(pred_entailment.argmax(dim=-1).cpu().numpy().tolist())\n",
    "            all_true_entailment.extend(entailment_labels.numpy().tolist())\n",
    "    \n",
    "    # Reload metrics for each epoch to avoid accumulation\n",
    "    psr = load(\"pearsonr\")\n",
    "    acc = load(\"accuracy\")\n",
    "    \n",
    "    pearson_corr = psr.compute(predictions=all_pred_relatedness, references=all_true_relatedness)[\"pearsonr\"] # Write your code here\n",
    "    accuracy = acc.compute(predictions=all_pred_entailment, references=all_true_entailment)[\"accuracy\"] # Write your code here\n",
    "    # print(f\"F1 Score: {f1.compute()}\")\n",
    "    \n",
    "    print(f\"[BERT] Epoch {ep+1}/{epochs} - Pearson Correlation: {pearson_corr:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    if pearson_corr + accuracy > best_score:\n",
    "        best_score = pearson_corr + accuracy\n",
    "        torch.save(model.state_dict(), f'./saved_models/best_model.ckpt')\n",
    "        print(f\"[BERT] Best model saved with score: {best_score:.4f}\")\n",
    "    \n",
    "    print()  # Add blank line between epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cell 23**: BERT 模型測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing BERT-base Model on Test Set\n",
      "✅ 使用 SICK 測試集進行評估\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[BERT] Final Evaluation: 100%|██████████| 614/614 [00:03<00:00, 164.09it/s]\n",
      "[BERT] Final Evaluation: 100%|██████████| 614/614 [00:03<00:00, 164.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[BERT Multi-task] Test Set Performance:\n",
      "  Pearson Correlation (Relatedness): 0.8853\n",
      "  Accuracy (Entailment):              0.8732\n",
      "  Combined Score:                     1.7585\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Testing BERT-base Model on Test Set\")\n",
    "print(\"✅ 使用 SICK 測試集進行評估\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model = MultiLabelModel().to(device)\n",
    "model.load_state_dict(torch.load(f\"./saved_models/best_model.ckpt\", weights_only=True))\n",
    "\n",
    "# Use test set for evaluation\n",
    "pbar = tqdm(dl_test, desc=\"[BERT] Final Evaluation\")\n",
    "model.eval()\n",
    "\n",
    "# TODO6: Write the test loop\n",
    "# Write your code here\n",
    "# We have loaded the best model with the highest evaluation score for you\n",
    "# Please implement the test loop to evaluate the model on the test dataset\n",
    "# We will have 10% of the total score for the test accuracy and pearson correlation\n",
    "\n",
    "# Write your code here\n",
    "all_pred_relatedness = []\n",
    "all_true_relatedness = []\n",
    "all_pred_entailment = []\n",
    "all_true_entailment = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in pbar:\n",
    "        encodings, relatedness_scores, entailment_labels = batch\n",
    "        encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        pred_relatedness, pred_entailment = model(**encodings)\n",
    "        \n",
    "        # Store predictions and ground truth\n",
    "        all_pred_relatedness.extend(pred_relatedness.cpu().numpy())\n",
    "        all_true_relatedness.extend(relatedness_scores.numpy())\n",
    "        all_pred_entailment.extend(pred_entailment.argmax(dim=-1).cpu().numpy())\n",
    "        all_true_entailment.extend(entailment_labels.numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "psr = load(\"pearsonr\")\n",
    "acc = load(\"accuracy\")\n",
    "\n",
    "test_pearson_corr = psr.compute(predictions=all_pred_relatedness, references=all_true_relatedness)['pearsonr']\n",
    "test_accuracy = acc.compute(predictions=all_pred_entailment, references=all_true_entailment)['accuracy']\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"[BERT Multi-task] Test Set Performance:\")\n",
    "print(f\"  Pearson Correlation (Relatedness): {test_pearson_corr:.4f}\")\n",
    "print(f\"  Accuracy (Entailment):              {test_accuracy:.4f}\")\n",
    "print(f\"  Combined Score:                     {test_pearson_corr + test_accuracy:.4f}\")\n",
    "print(f\"{'=' * 60}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cell 25**: 問題 1 - RoBERTa 模型訓練與測試 (共 2 個 code cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training RoBERTa-base Multi-output Model\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[RoBERTa] Training epoch [1/10]: 100%|██████████| 555/555 [00:20<00:00, 27.19it/s, loss=0.908]\n",
      "[RoBERTa] Training epoch [1/10]: 100%|██████████| 555/555 [00:20<00:00, 27.19it/s, loss=0.908]\n",
      "[RoBERTa] Validation epoch [1/10]: 100%|██████████| 62/62 [00:00<00:00, 184.84it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RoBERTa] Epoch 1/10 - Pearson Correlation: 0.8670, Accuracy: 0.8525\n",
      "[RoBERTa] Best model saved with score: 1.7195\n",
      "\n",
      "[RoBERTa] Best model saved with score: 1.7195\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RoBERTa] Training epoch [2/10]: 100%|██████████| 555/555 [00:26<00:00, 20.88it/s, loss=0.393]\n",
      "[RoBERTa] Training epoch [2/10]: 100%|██████████| 555/555 [00:26<00:00, 20.88it/s, loss=0.393]\n",
      "[RoBERTa] Validation epoch [2/10]: 100%|██████████| 62/62 [00:00<00:00, 173.82it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RoBERTa] Epoch 2/10 - Pearson Correlation: 0.8666, Accuracy: 0.8768\n",
      "[RoBERTa] Best model saved with score: 1.7434\n",
      "\n",
      "[RoBERTa] Best model saved with score: 1.7434\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RoBERTa] Training epoch [3/10]: 100%|██████████| 555/555 [00:24<00:00, 22.74it/s, loss=0.256] \n",
      "[RoBERTa] Training epoch [3/10]: 100%|██████████| 555/555 [00:24<00:00, 22.74it/s, loss=0.256]\n",
      "[RoBERTa] Validation epoch [3/10]: 100%|██████████| 62/62 [00:00<00:00, 177.33it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RoBERTa] Epoch 3/10 - Pearson Correlation: 0.8806, Accuracy: 0.8707\n",
      "[RoBERTa] Best model saved with score: 1.7513\n",
      "\n",
      "[RoBERTa] Best model saved with score: 1.7513\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RoBERTa] Training epoch [4/10]: 100%|██████████| 555/555 [00:26<00:00, 20.88it/s, loss=0.477] \n",
      "[RoBERTa] Training epoch [4/10]: 100%|██████████| 555/555 [00:26<00:00, 20.88it/s, loss=0.477]\n",
      "[RoBERTa] Validation epoch [4/10]: 100%|██████████| 62/62 [00:00<00:00, 167.99it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RoBERTa] Epoch 4/10 - Pearson Correlation: 0.8883, Accuracy: 0.8727\n",
      "[RoBERTa] Best model saved with score: 1.7610\n",
      "\n",
      "[RoBERTa] Best model saved with score: 1.7610\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RoBERTa] Training epoch [5/10]: 100%|██████████| 555/555 [00:26<00:00, 20.91it/s, loss=0.15]  \n",
      "[RoBERTa] Training epoch [5/10]: 100%|██████████| 555/555 [00:26<00:00, 20.91it/s, loss=0.15] \n",
      "[RoBERTa] Validation epoch [5/10]: 100%|██████████| 62/62 [00:00<00:00, 175.22it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RoBERTa] Epoch 5/10 - Pearson Correlation: 0.8844, Accuracy: 0.8424\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RoBERTa] Training epoch [6/10]: 100%|██████████| 555/555 [00:26<00:00, 20.91it/s, loss=0.155] \n",
      "[RoBERTa] Training epoch [6/10]: 100%|██████████| 555/555 [00:26<00:00, 20.91it/s, loss=0.155]\n",
      "[RoBERTa] Validation epoch [6/10]: 100%|██████████| 62/62 [00:00<00:00, 174.06it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RoBERTa] Epoch 6/10 - Pearson Correlation: 0.8856, Accuracy: 0.8545\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RoBERTa] Training epoch [7/10]: 100%|██████████| 555/555 [00:26<00:00, 20.89it/s, loss=0.351] \n",
      "[RoBERTa] Training epoch [7/10]: 100%|██████████| 555/555 [00:26<00:00, 20.89it/s, loss=0.351]\n",
      "[RoBERTa] Validation epoch [7/10]: 100%|██████████| 62/62 [00:00<00:00, 169.30it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RoBERTa] Epoch 7/10 - Pearson Correlation: 0.8873, Accuracy: 0.8626\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RoBERTa] Training epoch [8/10]: 100%|██████████| 555/555 [00:26<00:00, 20.92it/s, loss=0.299] \n",
      "[RoBERTa] Training epoch [8/10]: 100%|██████████| 555/555 [00:26<00:00, 20.92it/s, loss=0.299]\n",
      "[RoBERTa] Validation epoch [8/10]: 100%|██████████| 62/62 [00:00<00:00, 175.15it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RoBERTa] Epoch 8/10 - Pearson Correlation: 0.8934, Accuracy: 0.8667\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RoBERTa] Training epoch [9/10]: 100%|██████████| 555/555 [00:26<00:00, 20.94it/s, loss=0.826] \n",
      "[RoBERTa] Training epoch [9/10]: 100%|██████████| 555/555 [00:26<00:00, 20.94it/s, loss=0.826]\n",
      "[RoBERTa] Validation epoch [9/10]: 100%|██████████| 62/62 [00:00<00:00, 175.27it/s]\n",
      "[RoBERTa] Validation epoch [9/10]: 100%|██████████| 62/62 [00:00<00:00, 175.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RoBERTa] Epoch 9/10 - Pearson Correlation: 0.8806, Accuracy: 0.8808\n",
      "[RoBERTa] Best model saved with score: 1.7614\n",
      "\n",
      "[RoBERTa] Best model saved with score: 1.7614\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RoBERTa] Training epoch [10/10]: 100%|██████████| 555/555 [00:26<00:00, 20.65it/s, loss=0.158] \n",
      "[RoBERTa] Training epoch [10/10]: 100%|██████████| 555/555 [00:26<00:00, 20.65it/s, loss=0.158]\n",
      "[RoBERTa] Validation epoch [10/10]: 100%|██████████| 62/62 [00:00<00:00, 177.35it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RoBERTa] Epoch 10/10 - Pearson Correlation: 0.8939, Accuracy: 0.8727\n",
      "[RoBERTa] Best model saved with score: 1.7667\n",
      "\n",
      "[RoBERTa] Best model saved with score: 1.7667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Training RoBERTa-base Multi-output Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model_roberta = MultiLabelModelRoBERTa().to(device)\n",
    "optimizer_roberta = AdamW(model_roberta.parameters(), lr=lr)\n",
    "\n",
    "best_score_roberta = 0.0\n",
    "for ep in range(epochs):\n",
    "    pbar = tqdm(dl_train_roberta)\n",
    "    pbar.set_description(f\"[RoBERTa] Training epoch [{ep+1}/{epochs}]\")\n",
    "    model_roberta.train()\n",
    "    \n",
    "    for batch in pbar:\n",
    "        encodings, relatedness_scores, entailment_labels = batch\n",
    "        encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "        relatedness_scores = relatedness_scores.to(device)\n",
    "        entailment_labels = entailment_labels.to(device)\n",
    "        \n",
    "        optimizer_roberta.zero_grad()\n",
    "        pred_relatedness, pred_entailment = model_roberta(**encodings)\n",
    "        \n",
    "        loss_relatedness = relatedness_loss_fn(pred_relatedness, relatedness_scores)\n",
    "        loss_entailment = entailment_loss_fn(pred_entailment, entailment_labels)\n",
    "        loss = loss_relatedness + loss_entailment\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_roberta.step()\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    pbar = tqdm(dl_validation_roberta)\n",
    "    pbar.set_description(f\"[RoBERTa] Validation epoch [{ep+1}/{epochs}]\")\n",
    "    model_roberta.eval()\n",
    "    \n",
    "    all_pred_relatedness = []\n",
    "    all_true_relatedness = []\n",
    "    all_pred_entailment = []\n",
    "    all_true_entailment = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            encodings, relatedness_scores, entailment_labels = batch\n",
    "            encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "            \n",
    "            pred_relatedness, pred_entailment = model_roberta(**encodings)\n",
    "            \n",
    "            all_pred_relatedness.extend(pred_relatedness.cpu().numpy().tolist())\n",
    "            all_true_relatedness.extend(relatedness_scores.numpy().tolist())\n",
    "            all_pred_entailment.extend(pred_entailment.argmax(dim=-1).cpu().numpy().tolist())\n",
    "            all_true_entailment.extend(entailment_labels.numpy().tolist())\n",
    "    \n",
    "    # Reload metrics for each epoch to avoid accumulation\n",
    "    psr_roberta = load(\"pearsonr\")\n",
    "    acc_roberta = load(\"accuracy\")\n",
    "    \n",
    "    pearson_corr = psr_roberta.compute(predictions=all_pred_relatedness, references=all_true_relatedness)[\"pearsonr\"]\n",
    "    accuracy = acc_roberta.compute(predictions=all_pred_entailment, references=all_true_entailment)[\"accuracy\"]\n",
    "    \n",
    "    print(f\"[RoBERTa] Epoch {ep+1}/{epochs} - Pearson Correlation: {pearson_corr:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    if pearson_corr + accuracy > best_score_roberta:\n",
    "        best_score_roberta = pearson_corr + accuracy\n",
    "        torch.save(model_roberta.state_dict(), f'./saved_models/best_model_roberta.ckpt')\n",
    "        print(f\"[RoBERTa] Best model saved with score: {best_score_roberta:.4f}\")\n",
    "    \n",
    "    print()  # Add blank line between epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing RoBERTa-base Model on Test Set\n",
      "✅ 使用 SICK 測試集進行評估\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[RoBERTa] Final Evaluation: 100%|██████████| 614/614 [00:03<00:00, 188.83it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[RoBERTa] Test Set Performance:\n",
      "  Pearson Correlation: 0.8909\n",
      "  Accuracy: 0.8728\n",
      "  Combined Score: 1.7637\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test RoBERTa Model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Testing RoBERTa-base Model on Test Set\")\n",
    "print(\"✅ 使用 SICK 測試集進行評估\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model_roberta = MultiLabelModelRoBERTa().to(device)\n",
    "model_roberta.load_state_dict(torch.load(f\"./saved_models/best_model_roberta.ckpt\", weights_only=True))\n",
    "\n",
    "pbar = tqdm(dl_test_roberta, desc=\"[RoBERTa] Final Evaluation\")\n",
    "model_roberta.eval()\n",
    "\n",
    "all_pred_relatedness = []\n",
    "all_true_relatedness = []\n",
    "all_pred_entailment = []\n",
    "all_true_entailment = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in pbar:\n",
    "        encodings, relatedness_scores, entailment_labels = batch\n",
    "        encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "        \n",
    "        pred_relatedness, pred_entailment = model_roberta(**encodings)\n",
    "        \n",
    "        all_pred_relatedness.extend(pred_relatedness.cpu().numpy().tolist())\n",
    "        all_true_relatedness.extend(relatedness_scores.numpy().tolist())\n",
    "        all_pred_entailment.extend(pred_entailment.argmax(dim=-1).cpu().numpy().tolist())\n",
    "        all_true_entailment.extend(entailment_labels.numpy().tolist())\n",
    "\n",
    "# Reload metrics to avoid accumulation\n",
    "psr_roberta_test = load(\"pearsonr\")\n",
    "acc_roberta_test = load(\"accuracy\")\n",
    "\n",
    "roberta_test_pearson = psr_roberta_test.compute(predictions=all_pred_relatedness, references=all_true_relatedness)[\"pearsonr\"]\n",
    "roberta_test_accuracy = acc_roberta_test.compute(predictions=all_pred_entailment, references=all_true_entailment)[\"accuracy\"]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"[RoBERTa] Test Set Performance:\")\n",
    "print(f\"  Pearson Correlation: {roberta_test_pearson:.4f}\")\n",
    "print(f\"  Accuracy: {roberta_test_accuracy:.4f}\")\n",
    "print(f\"  Combined Score: {roberta_test_pearson + roberta_test_accuracy:.4f}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cell 28**: 問題 2 - GPT-2 模型訓練與測試 (共 2 個 code cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training GPT-2 Multi-output Model\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[GPT-2] Training epoch [1/10]: 100%|██████████| 555/555 [00:23<00:00, 23.68it/s, loss=0.551]\n",
      "[GPT-2] Training epoch [1/10]: 100%|██████████| 555/555 [00:23<00:00, 23.68it/s, loss=0.551]\n",
      "[GPT-2] Validation epoch [1/10]: 100%|██████████| 62/62 [00:00<00:00, 116.04it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPT-2] Epoch 1/10 - Pearson Correlation: 0.8230, Accuracy: 0.8303\n",
      "[GPT-2] Best model saved with score: 1.6533\n",
      "\n",
      "[GPT-2] Best model saved with score: 1.6533\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[GPT-2] Training epoch [2/10]: 100%|██████████| 555/555 [00:27<00:00, 20.36it/s, loss=1.07] \n",
      "[GPT-2] Training epoch [2/10]: 100%|██████████| 555/555 [00:27<00:00, 20.36it/s, loss=1.07] \n",
      "[GPT-2] Validation epoch [2/10]: 100%|██████████| 62/62 [00:00<00:00, 113.82it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPT-2] Epoch 2/10 - Pearson Correlation: 0.8422, Accuracy: 0.8545\n",
      "[GPT-2] Best model saved with score: 1.6968\n",
      "\n",
      "[GPT-2] Best model saved with score: 1.6968\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[GPT-2] Training epoch [3/10]: 100%|██████████| 555/555 [00:27<00:00, 20.38it/s, loss=0.607]\n",
      "[GPT-2] Training epoch [3/10]: 100%|██████████| 555/555 [00:27<00:00, 20.38it/s, loss=0.607]\n",
      "[GPT-2] Validation epoch [3/10]: 100%|██████████| 62/62 [00:00<00:00, 114.03it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPT-2] Epoch 3/10 - Pearson Correlation: 0.8637, Accuracy: 0.8505\n",
      "[GPT-2] Best model saved with score: 1.7142\n",
      "\n",
      "[GPT-2] Best model saved with score: 1.7142\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[GPT-2] Training epoch [4/10]: 100%|██████████| 555/555 [00:27<00:00, 20.31it/s, loss=0.532]\n",
      "[GPT-2] Training epoch [4/10]: 100%|██████████| 555/555 [00:27<00:00, 20.31it/s, loss=0.532]\n",
      "[GPT-2] Validation epoch [4/10]: 100%|██████████| 62/62 [00:00<00:00, 113.53it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPT-2] Epoch 4/10 - Pearson Correlation: 0.8548, Accuracy: 0.8606\n",
      "[GPT-2] Best model saved with score: 1.7154\n",
      "\n",
      "[GPT-2] Best model saved with score: 1.7154\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[GPT-2] Training epoch [5/10]: 100%|██████████| 555/555 [00:23<00:00, 23.25it/s, loss=0.271] \n",
      "[GPT-2] Training epoch [5/10]: 100%|██████████| 555/555 [00:23<00:00, 23.25it/s, loss=0.271]\n",
      "[GPT-2] Validation epoch [5/10]: 100%|██████████| 62/62 [00:00<00:00, 116.55it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPT-2] Epoch 5/10 - Pearson Correlation: 0.8580, Accuracy: 0.8485\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[GPT-2] Training epoch [6/10]: 100%|██████████| 555/555 [00:26<00:00, 21.05it/s, loss=0.892] \n",
      "[GPT-2] Training epoch [6/10]: 100%|██████████| 555/555 [00:26<00:00, 21.05it/s, loss=0.892]\n",
      "[GPT-2] Validation epoch [6/10]: 100%|██████████| 62/62 [00:00<00:00, 113.98it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPT-2] Epoch 6/10 - Pearson Correlation: 0.8710, Accuracy: 0.8667\n",
      "[GPT-2] Best model saved with score: 1.7377\n",
      "\n",
      "[GPT-2] Best model saved with score: 1.7377\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[GPT-2] Training epoch [7/10]: 100%|██████████| 555/555 [00:26<00:00, 21.02it/s, loss=0.173] \n",
      "[GPT-2] Training epoch [7/10]: 100%|██████████| 555/555 [00:26<00:00, 21.02it/s, loss=0.173]\n",
      "[GPT-2] Validation epoch [7/10]: 100%|██████████| 62/62 [00:00<00:00, 113.60it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPT-2] Epoch 7/10 - Pearson Correlation: 0.8663, Accuracy: 0.8646\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[GPT-2] Training epoch [8/10]: 100%|██████████| 555/555 [00:26<00:00, 20.92it/s, loss=0.469] \n",
      "[GPT-2] Training epoch [8/10]: 100%|██████████| 555/555 [00:26<00:00, 20.92it/s, loss=0.469]\n",
      "[GPT-2] Validation epoch [8/10]: 100%|██████████| 62/62 [00:00<00:00, 113.18it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPT-2] Epoch 8/10 - Pearson Correlation: 0.8635, Accuracy: 0.8687\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[GPT-2] Training epoch [9/10]: 100%|██████████| 555/555 [00:26<00:00, 20.93it/s, loss=0.274] \n",
      "[GPT-2] Training epoch [9/10]: 100%|██████████| 555/555 [00:26<00:00, 20.93it/s, loss=0.274]\n",
      "[GPT-2] Validation epoch [9/10]: 100%|██████████| 62/62 [00:00<00:00, 113.98it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPT-2] Epoch 9/10 - Pearson Correlation: 0.8697, Accuracy: 0.8646\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[GPT-2] Training epoch [10/10]: 100%|██████████| 555/555 [00:26<00:00, 20.95it/s, loss=0.0791]\n",
      "[GPT-2] Training epoch [10/10]: 100%|██████████| 555/555 [00:26<00:00, 20.95it/s, loss=0.0791]\n",
      "[GPT-2] Validation epoch [10/10]: 100%|██████████| 62/62 [00:00<00:00, 113.51it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPT-2] Epoch 10/10 - Pearson Correlation: 0.8590, Accuracy: 0.8727\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Training GPT-2 Multi-output Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model_gpt2 = MultiLabelModelGPT2().to(device)\n",
    "optimizer_gpt2 = AdamW(model_gpt2.parameters(), lr=lr)\n",
    "\n",
    "best_score_gpt2 = 0.0\n",
    "for ep in range(epochs):\n",
    "    pbar = tqdm(dl_train_gpt2)\n",
    "    pbar.set_description(f\"[GPT-2] Training epoch [{ep+1}/{epochs}]\")\n",
    "    model_gpt2.train()\n",
    "    \n",
    "    for batch in pbar:\n",
    "        encodings, relatedness_scores, entailment_labels = batch\n",
    "        encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "        relatedness_scores = relatedness_scores.to(device)\n",
    "        entailment_labels = entailment_labels.to(device)\n",
    "        \n",
    "        optimizer_gpt2.zero_grad()\n",
    "        pred_relatedness, pred_entailment = model_gpt2(**encodings)\n",
    "        \n",
    "        loss_relatedness = relatedness_loss_fn(pred_relatedness, relatedness_scores)\n",
    "        loss_entailment = entailment_loss_fn(pred_entailment, entailment_labels)\n",
    "        loss = loss_relatedness + loss_entailment\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_gpt2.step()\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    pbar = tqdm(dl_validation_gpt2)\n",
    "    pbar.set_description(f\"[GPT-2] Validation epoch [{ep+1}/{epochs}]\")\n",
    "    model_gpt2.eval()\n",
    "    \n",
    "    all_pred_relatedness = []\n",
    "    all_true_relatedness = []\n",
    "    all_pred_entailment = []\n",
    "    all_true_entailment = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            encodings, relatedness_scores, entailment_labels = batch\n",
    "            encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "            \n",
    "            pred_relatedness, pred_entailment = model_gpt2(**encodings)\n",
    "            \n",
    "            all_pred_relatedness.extend(pred_relatedness.cpu().numpy().tolist())\n",
    "            all_true_relatedness.extend(relatedness_scores.numpy().tolist())\n",
    "            all_pred_entailment.extend(pred_entailment.argmax(dim=-1).cpu().numpy().tolist())\n",
    "            all_true_entailment.extend(entailment_labels.numpy().tolist())\n",
    "    \n",
    "    # Reload metrics for each epoch to avoid accumulation\n",
    "    psr_gpt2 = load(\"pearsonr\")\n",
    "    acc_gpt2 = load(\"accuracy\")\n",
    "    \n",
    "    pearson_corr = psr_gpt2.compute(predictions=all_pred_relatedness, references=all_true_relatedness)[\"pearsonr\"]\n",
    "    accuracy = acc_gpt2.compute(predictions=all_pred_entailment, references=all_true_entailment)[\"accuracy\"]\n",
    "    \n",
    "    print(f\"[GPT-2] Epoch {ep+1}/{epochs} - Pearson Correlation: {pearson_corr:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    if pearson_corr + accuracy > best_score_gpt2:\n",
    "        best_score_gpt2 = pearson_corr + accuracy\n",
    "        torch.save(model_gpt2.state_dict(), f'./saved_models/best_model_gpt2.ckpt')\n",
    "        print(f\"[GPT-2] Best model saved with score: {best_score_gpt2:.4f}\")\n",
    "    \n",
    "    print()  # Add blank line between epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing GPT-2 Model on Test Set\n",
      "✅ 使用 SICK 測試集進行評估\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[GPT-2] Final Evaluation: 100%|██████████| 614/614 [00:05<00:00, 115.31it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[GPT-2] Test Set Performance:\n",
      "  Pearson Correlation: 0.8703\n",
      "  Accuracy: 0.8801\n",
      "  Combined Score: 1.7504\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test GPT-2 Model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Testing GPT-2 Model on Test Set\")\n",
    "print(\"✅ 使用 SICK 測試集進行評估\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model_gpt2 = MultiLabelModelGPT2().to(device)\n",
    "model_gpt2.load_state_dict(torch.load(f\"./saved_models/best_model_gpt2.ckpt\", weights_only=True))\n",
    "\n",
    "pbar = tqdm(dl_test_gpt2, desc=\"[GPT-2] Final Evaluation\")\n",
    "model_gpt2.eval()\n",
    "\n",
    "all_pred_relatedness = []\n",
    "all_true_relatedness = []\n",
    "all_pred_entailment = []\n",
    "all_true_entailment = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in pbar:\n",
    "        encodings, relatedness_scores, entailment_labels = batch\n",
    "        encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "        \n",
    "        pred_relatedness, pred_entailment = model_gpt2(**encodings)\n",
    "        \n",
    "        all_pred_relatedness.extend(pred_relatedness.cpu().numpy().tolist())\n",
    "        all_true_relatedness.extend(relatedness_scores.numpy().tolist())\n",
    "        all_pred_entailment.extend(pred_entailment.argmax(dim=-1).cpu().numpy().tolist())\n",
    "        all_true_entailment.extend(entailment_labels.numpy().tolist())\n",
    "\n",
    "# Reload metrics to avoid accumulation\n",
    "psr_gpt2_test = load(\"pearsonr\")\n",
    "acc_gpt2_test = load(\"accuracy\")\n",
    "\n",
    "gpt2_test_pearson = psr_gpt2_test.compute(predictions=all_pred_relatedness, references=all_true_relatedness)[\"pearsonr\"]\n",
    "gpt2_test_accuracy = acc_gpt2_test.compute(predictions=all_pred_entailment, references=all_true_entailment)[\"accuracy\"]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"[GPT-2] Test Set Performance:\")\n",
    "print(f\"  Pearson Correlation: {gpt2_test_pearson:.4f}\")\n",
    "print(f\"  Accuracy: {gpt2_test_accuracy:.4f}\")\n",
    "print(f\"  Combined Score: {gpt2_test_pearson + gpt2_test_accuracy:.4f}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cell 31**: 問題 3 - 單任務 BERT 模型訓練與測試 (共 3 個 code cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training Single-task Model: Relatedness Score Prediction\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Single-Relatedness] Training epoch [1/10]: 100%|██████████| 555/555 [00:22<00:00, 24.21it/s, loss=0.178] \n",
      "[Single-Relatedness] Training epoch [1/10]: 100%|██████████| 555/555 [00:22<00:00, 24.21it/s, loss=0.178]\n",
      "[Single-Relatedness] Validation epoch [1/10]: 100%|██████████| 62/62 [00:00<00:00, 158.43it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Single-Relatedness] Epoch 1/10 - Pearson Correlation: 0.8665\n",
      "[Single-Relatedness] Best model saved with Pearson: 0.8665\n",
      "\n",
      "[Single-Relatedness] Best model saved with Pearson: 0.8665\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Single-Relatedness] Training epoch [2/10]: 100%|██████████| 555/555 [00:26<00:00, 20.99it/s, loss=0.237] \n",
      "[Single-Relatedness] Training epoch [2/10]: 100%|██████████| 555/555 [00:26<00:00, 20.99it/s, loss=0.237]\n",
      "[Single-Relatedness] Validation epoch [2/10]: 100%|██████████| 62/62 [00:00<00:00, 153.16it/s]\n",
      "[Single-Relatedness] Validation epoch [2/10]: 100%|██████████| 62/62 [00:00<00:00, 153.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Single-Relatedness] Epoch 2/10 - Pearson Correlation: 0.8756\n",
      "[Single-Relatedness] Best model saved with Pearson: 0.8756\n",
      "\n",
      "[Single-Relatedness] Best model saved with Pearson: 0.8756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Single-Relatedness] Training epoch [3/10]: 100%|██████████| 555/555 [00:25<00:00, 21.98it/s, loss=0.0552]\n",
      "[Single-Relatedness] Training epoch [3/10]: 100%|██████████| 555/555 [00:25<00:00, 21.98it/s, loss=0.0552]\n",
      "[Single-Relatedness] Validation epoch [3/10]: 100%|██████████| 62/62 [00:00<00:00, 160.46it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Single-Relatedness] Epoch 3/10 - Pearson Correlation: 0.8699\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Single-Relatedness] Training epoch [4/10]: 100%|██████████| 555/555 [00:26<00:00, 21.03it/s, loss=0.191] \n",
      "[Single-Relatedness] Training epoch [4/10]: 100%|██████████| 555/555 [00:26<00:00, 21.03it/s, loss=0.191] \n",
      "[Single-Relatedness] Validation epoch [4/10]: 100%|██████████| 62/62 [00:00<00:00, 154.45it/s]\n",
      "[Single-Relatedness] Validation epoch [4/10]: 100%|██████████| 62/62 [00:00<00:00, 154.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Single-Relatedness] Epoch 4/10 - Pearson Correlation: 0.8810\n",
      "[Single-Relatedness] Best model saved with Pearson: 0.8810\n",
      "\n",
      "[Single-Relatedness] Best model saved with Pearson: 0.8810\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Single-Relatedness] Training epoch [5/10]: 100%|██████████| 555/555 [00:22<00:00, 24.22it/s, loss=0.0199]\n",
      "[Single-Relatedness] Training epoch [5/10]: 100%|██████████| 555/555 [00:22<00:00, 24.22it/s, loss=0.0199]\n",
      "[Single-Relatedness] Validation epoch [5/10]: 100%|██████████| 62/62 [00:00<00:00, 159.01it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Single-Relatedness] Epoch 5/10 - Pearson Correlation: 0.8852\n",
      "[Single-Relatedness] Best model saved with Pearson: 0.8852\n",
      "\n",
      "[Single-Relatedness] Best model saved with Pearson: 0.8852\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Single-Relatedness] Training epoch [6/10]: 100%|██████████| 555/555 [00:26<00:00, 20.98it/s, loss=0.0597] \n",
      "[Single-Relatedness] Training epoch [6/10]: 100%|██████████| 555/555 [00:26<00:00, 20.98it/s, loss=0.0597]\n",
      "[Single-Relatedness] Validation epoch [6/10]: 100%|██████████| 62/62 [00:00<00:00, 152.99it/s]\n",
      "[Single-Relatedness] Validation epoch [6/10]: 100%|██████████| 62/62 [00:00<00:00, 152.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Single-Relatedness] Epoch 6/10 - Pearson Correlation: 0.8859\n",
      "[Single-Relatedness] Best model saved with Pearson: 0.8859\n",
      "\n",
      "[Single-Relatedness] Best model saved with Pearson: 0.8859\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Single-Relatedness] Training epoch [7/10]: 100%|██████████| 555/555 [00:26<00:00, 21.02it/s, loss=0.132]  \n",
      "[Single-Relatedness] Training epoch [7/10]: 100%|██████████| 555/555 [00:26<00:00, 21.02it/s, loss=0.132]\n",
      "[Single-Relatedness] Validation epoch [7/10]: 100%|██████████| 62/62 [00:00<00:00, 154.98it/s]\n",
      "[Single-Relatedness] Validation epoch [7/10]: 100%|██████████| 62/62 [00:00<00:00, 154.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Single-Relatedness] Epoch 7/10 - Pearson Correlation: 0.8829\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Single-Relatedness] Training epoch [8/10]: 100%|██████████| 555/555 [00:26<00:00, 21.04it/s, loss=0.066]  \n",
      "[Single-Relatedness] Training epoch [8/10]: 100%|██████████| 555/555 [00:26<00:00, 21.04it/s, loss=0.066] \n",
      "[Single-Relatedness] Validation epoch [8/10]: 100%|██████████| 62/62 [00:00<00:00, 154.13it/s]\n",
      "[Single-Relatedness] Validation epoch [8/10]: 100%|██████████| 62/62 [00:00<00:00, 154.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Single-Relatedness] Epoch 8/10 - Pearson Correlation: 0.8862\n",
      "[Single-Relatedness] Best model saved with Pearson: 0.8862\n",
      "\n",
      "[Single-Relatedness] Best model saved with Pearson: 0.8862\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Single-Relatedness] Training epoch [9/10]: 100%|██████████| 555/555 [00:26<00:00, 21.05it/s, loss=0.0565] \n",
      "[Single-Relatedness] Training epoch [9/10]: 100%|██████████| 555/555 [00:26<00:00, 21.05it/s, loss=0.0565]\n",
      "[Single-Relatedness] Validation epoch [9/10]: 100%|██████████| 62/62 [00:00<00:00, 148.67it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Single-Relatedness] Epoch 9/10 - Pearson Correlation: 0.8745\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Single-Relatedness] Training epoch [10/10]: 100%|██████████| 555/555 [00:26<00:00, 21.02it/s, loss=0.072]  \n",
      "[Single-Relatedness] Training epoch [10/10]: 100%|██████████| 555/555 [00:26<00:00, 21.02it/s, loss=0.072] \n",
      "[Single-Relatedness] Validation epoch [10/10]: 100%|██████████| 62/62 [00:00<00:00, 154.21it/s]\n",
      "[Single-Relatedness] Validation epoch [10/10]: 100%|██████████| 62/62 [00:00<00:00, 154.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Single-Relatedness] Epoch 10/10 - Pearson Correlation: 0.8817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Training Single-task Model: Relatedness Score Prediction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model_relatedness = SingleTaskRelatednessModel().to(device)\n",
    "optimizer_relatedness = AdamW(model_relatedness.parameters(), lr=lr)\n",
    "\n",
    "best_pearson = 0.0\n",
    "for ep in range(epochs):\n",
    "    pbar = tqdm(dl_train)\n",
    "    pbar.set_description(f\"[Single-Relatedness] Training epoch [{ep+1}/{epochs}]\")\n",
    "    model_relatedness.train()\n",
    "    \n",
    "    for batch in pbar:\n",
    "        encodings, relatedness_scores, _ = batch\n",
    "        encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "        relatedness_scores = relatedness_scores.to(device)\n",
    "        \n",
    "        optimizer_relatedness.zero_grad()\n",
    "        pred_relatedness = model_relatedness(**encodings)\n",
    "        \n",
    "        loss = relatedness_loss_fn(pred_relatedness, relatedness_scores)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_relatedness.step()\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    pbar = tqdm(dl_validation)\n",
    "    pbar.set_description(f\"[Single-Relatedness] Validation epoch [{ep+1}/{epochs}]\")\n",
    "    model_relatedness.eval()\n",
    "    \n",
    "    all_pred_relatedness = []\n",
    "    all_true_relatedness = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            encodings, relatedness_scores, _ = batch\n",
    "            encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "            \n",
    "            pred_relatedness = model_relatedness(**encodings)\n",
    "            \n",
    "            all_pred_relatedness.extend(pred_relatedness.cpu().numpy().tolist())\n",
    "            all_true_relatedness.extend(relatedness_scores.numpy().tolist())\n",
    "    \n",
    "    # Reload metrics for each epoch to avoid accumulation\n",
    "    psr_rel = load(\"pearsonr\")\n",
    "    \n",
    "    pearson_corr = psr_rel.compute(predictions=all_pred_relatedness, references=all_true_relatedness)[\"pearsonr\"]\n",
    "    \n",
    "    print(f\"[Single-Relatedness] Epoch {ep+1}/{epochs} - Pearson Correlation: {pearson_corr:.4f}\")\n",
    "    \n",
    "    if pearson_corr > best_pearson:\n",
    "        best_pearson = pearson_corr\n",
    "        torch.save(model_relatedness.state_dict(), f'./saved_models/best_model_relatedness.ckpt')\n",
    "        print(f\"[Single-Relatedness] Best model saved with Pearson: {best_pearson:.4f}\")\n",
    "    \n",
    "    print()  # Add blank line between epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training Single-task Model: Entailment Classification\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Single-Entailment] Training epoch [1/10]: 100%|██████████| 555/555 [00:22<00:00, 24.30it/s, loss=0.895] \n",
      "[Single-Entailment] Training epoch [1/10]: 100%|██████████| 555/555 [00:22<00:00, 24.30it/s, loss=0.895]\n",
      "[Single-Entailment] Validation epoch [1/10]: 100%|██████████| 62/62 [00:00<00:00, 158.61it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Single-Entailment] Epoch 1/10 - Accuracy: 0.8505\n",
      "[Single-Entailment] Best model saved with Accuracy: 0.8505\n",
      "\n",
      "[Single-Entailment] Best model saved with Accuracy: 0.8505\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Single-Entailment] Training epoch [2/10]: 100%|██████████| 555/555 [00:26<00:00, 21.03it/s, loss=0.319] \n",
      "[Single-Entailment] Training epoch [2/10]: 100%|██████████| 555/555 [00:26<00:00, 21.03it/s, loss=0.319]\n",
      "[Single-Entailment] Validation epoch [2/10]: 100%|██████████| 62/62 [00:00<00:00, 153.77it/s]\n",
      "[Single-Entailment] Validation epoch [2/10]: 100%|██████████| 62/62 [00:00<00:00, 153.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Single-Entailment] Epoch 2/10 - Accuracy: 0.8283\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Single-Entailment] Training epoch [3/10]: 100%|██████████| 555/555 [00:26<00:00, 21.05it/s, loss=1.17]  \n",
      "[Single-Entailment] Training epoch [3/10]: 100%|██████████| 555/555 [00:26<00:00, 21.05it/s, loss=1.17] \n",
      "[Single-Entailment] Validation epoch [3/10]: 100%|██████████| 62/62 [00:00<00:00, 153.74it/s]\n",
      "[Single-Entailment] Validation epoch [3/10]: 100%|██████████| 62/62 [00:00<00:00, 153.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Single-Entailment] Epoch 3/10 - Accuracy: 0.8424\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Single-Entailment] Training epoch [4/10]: 100%|██████████| 555/555 [00:26<00:00, 21.02it/s, loss=0.0922] \n",
      "[Single-Entailment] Training epoch [4/10]: 100%|██████████| 555/555 [00:26<00:00, 21.02it/s, loss=0.0922]\n",
      "[Single-Entailment] Validation epoch [4/10]: 100%|██████████| 62/62 [00:00<00:00, 154.44it/s]\n",
      "[Single-Entailment] Validation epoch [4/10]: 100%|██████████| 62/62 [00:00<00:00, 154.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Single-Entailment] Epoch 4/10 - Accuracy: 0.8545\n",
      "[Single-Entailment] Best model saved with Accuracy: 0.8545\n",
      "\n",
      "[Single-Entailment] Best model saved with Accuracy: 0.8545\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Single-Entailment] Training epoch [5/10]: 100%|██████████| 555/555 [00:26<00:00, 20.96it/s, loss=0.0231] \n",
      "[Single-Entailment] Training epoch [5/10]: 100%|██████████| 555/555 [00:26<00:00, 20.96it/s, loss=0.0231]\n",
      "[Single-Entailment] Validation epoch [5/10]: 100%|██████████| 62/62 [00:00<00:00, 153.50it/s]\n",
      "[Single-Entailment] Validation epoch [5/10]: 100%|██████████| 62/62 [00:00<00:00, 153.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Single-Entailment] Epoch 5/10 - Accuracy: 0.8545\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Single-Entailment] Training epoch [6/10]: 100%|██████████| 555/555 [00:26<00:00, 21.02it/s, loss=0.0524] \n",
      "[Single-Entailment] Training epoch [6/10]: 100%|██████████| 555/555 [00:26<00:00, 21.02it/s, loss=0.0524]\n",
      "[Single-Entailment] Validation epoch [6/10]: 100%|██████████| 62/62 [00:00<00:00, 154.98it/s]\n",
      "[Single-Entailment] Validation epoch [6/10]: 100%|██████████| 62/62 [00:00<00:00, 154.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Single-Entailment] Epoch 6/10 - Accuracy: 0.8505\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Single-Entailment] Training epoch [7/10]: 100%|██████████| 555/555 [00:26<00:00, 21.06it/s, loss=0.03]   \n",
      "[Single-Entailment] Training epoch [7/10]: 100%|██████████| 555/555 [00:26<00:00, 21.06it/s, loss=0.03]  \n",
      "[Single-Entailment] Validation epoch [7/10]: 100%|██████████| 62/62 [00:00<00:00, 155.58it/s]\n",
      "[Single-Entailment] Validation epoch [7/10]: 100%|██████████| 62/62 [00:00<00:00, 155.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Single-Entailment] Epoch 7/10 - Accuracy: 0.8465\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Single-Entailment] Training epoch [8/10]: 100%|██████████| 555/555 [00:26<00:00, 21.05it/s, loss=0.574]  \n",
      "[Single-Entailment] Training epoch [8/10]: 100%|██████████| 555/555 [00:26<00:00, 21.05it/s, loss=0.574]\n",
      "[Single-Entailment] Validation epoch [8/10]: 100%|██████████| 62/62 [00:00<00:00, 154.03it/s]\n",
      "[Single-Entailment] Validation epoch [8/10]: 100%|██████████| 62/62 [00:00<00:00, 154.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Single-Entailment] Epoch 8/10 - Accuracy: 0.8707\n",
      "[Single-Entailment] Best model saved with Accuracy: 0.8707\n",
      "\n",
      "[Single-Entailment] Best model saved with Accuracy: 0.8707\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Single-Entailment] Training epoch [9/10]: 100%|██████████| 555/555 [00:27<00:00, 20.42it/s, loss=0.192]   \n",
      "[Single-Entailment] Training epoch [9/10]: 100%|██████████| 555/555 [00:27<00:00, 20.42it/s, loss=0.192]  \n",
      "[Single-Entailment] Validation epoch [9/10]: 100%|██████████| 62/62 [00:00<00:00, 155.02it/s]\n",
      "[Single-Entailment] Validation epoch [9/10]: 100%|██████████| 62/62 [00:00<00:00, 155.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Single-Entailment] Epoch 9/10 - Accuracy: 0.8566\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Single-Entailment] Training epoch [10/10]: 100%|██████████| 555/555 [00:27<00:00, 20.33it/s, loss=0.00135] \n",
      "[Single-Entailment] Training epoch [10/10]: 100%|██████████| 555/555 [00:27<00:00, 20.33it/s, loss=0.00135]\n",
      "[Single-Entailment] Validation epoch [10/10]: 100%|██████████| 62/62 [00:00<00:00, 154.82it/s]\n",
      "[Single-Entailment] Validation epoch [10/10]: 100%|██████████| 62/62 [00:00<00:00, 154.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Single-Entailment] Epoch 10/10 - Accuracy: 0.8485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Training Single-task Model: Entailment Classification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model_entailment = SingleTaskEntailmentModel().to(device)\n",
    "optimizer_entailment = AdamW(model_entailment.parameters(), lr=lr)\n",
    "\n",
    "best_accuracy = 0.0\n",
    "for ep in range(epochs):\n",
    "    pbar = tqdm(dl_train)\n",
    "    pbar.set_description(f\"[Single-Entailment] Training epoch [{ep+1}/{epochs}]\")\n",
    "    model_entailment.train()\n",
    "    \n",
    "    for batch in pbar:\n",
    "        encodings, _, entailment_labels = batch\n",
    "        encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "        entailment_labels = entailment_labels.to(device)\n",
    "        \n",
    "        optimizer_entailment.zero_grad()\n",
    "        pred_entailment = model_entailment(**encodings)\n",
    "        \n",
    "        loss = entailment_loss_fn(pred_entailment, entailment_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_entailment.step()\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    pbar = tqdm(dl_validation)\n",
    "    pbar.set_description(f\"[Single-Entailment] Validation epoch [{ep+1}/{epochs}]\")\n",
    "    model_entailment.eval()\n",
    "    \n",
    "    all_pred_entailment = []\n",
    "    all_true_entailment = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            encodings, _, entailment_labels = batch\n",
    "            encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "            \n",
    "            pred_entailment = model_entailment(**encodings)\n",
    "            \n",
    "            all_pred_entailment.extend(pred_entailment.argmax(dim=-1).cpu().numpy().tolist())\n",
    "            all_true_entailment.extend(entailment_labels.numpy().tolist())\n",
    "    \n",
    "    # Reload metrics for each epoch to avoid accumulation\n",
    "    acc_ent = load(\"accuracy\")\n",
    "    \n",
    "    accuracy = acc_ent.compute(predictions=all_pred_entailment, references=all_true_entailment)[\"accuracy\"]\n",
    "    \n",
    "    print(f\"[Single-Entailment] Epoch {ep+1}/{epochs} - Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model_entailment.state_dict(), f'./saved_models/best_model_entailment.ckpt')\n",
    "        print(f\"[Single-Entailment] Best model saved with Accuracy: {best_accuracy:.4f}\")\n",
    "    \n",
    "    print()  # Add blank line between epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing Single-task Models on Test Set\n",
      "✅ 使用 SICK 測試集進行評估\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Single-Relatedness] Final Evaluation: 100%|██████████| 614/614 [00:03<00:00, 166.32it/s]\n",
      "\n",
      "[Single-Entailment] Final Evaluation: 100%|██████████| 614/614 [00:03<00:00, 166.66it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[Single-task Models] Test Set Performance:\n",
      "  Relatedness - Pearson Correlation: 0.8907\n",
      "  Entailment - Accuracy: 0.8663\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test Single-task Models\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Testing Single-task Models on Test Set\")\n",
    "print(\"✅ 使用 SICK 測試集進行評估\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test Relatedness Model\n",
    "model_relatedness = SingleTaskRelatednessModel().to(device)\n",
    "model_relatedness.load_state_dict(torch.load(f\"./saved_models/best_model_relatedness.ckpt\", weights_only=True))\n",
    "model_relatedness.eval()\n",
    "\n",
    "all_pred_relatedness = []\n",
    "all_true_relatedness = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dl_test, desc=\"[Single-Relatedness] Final Evaluation\"):\n",
    "        encodings, relatedness_scores, _ = batch\n",
    "        encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "        \n",
    "        pred_relatedness = model_relatedness(**encodings)\n",
    "        \n",
    "        all_pred_relatedness.extend(pred_relatedness.cpu().numpy().tolist())\n",
    "        all_true_relatedness.extend(relatedness_scores.numpy().tolist())\n",
    "\n",
    "# Reload metrics to avoid accumulation\n",
    "psr_rel_test = load(\"pearsonr\")\n",
    "single_relatedness_pearson = psr_rel_test.compute(predictions=all_pred_relatedness, references=all_true_relatedness)[\"pearsonr\"]\n",
    "\n",
    "# Test Entailment Model\n",
    "model_entailment = SingleTaskEntailmentModel().to(device)\n",
    "model_entailment.load_state_dict(torch.load(f\"./saved_models/best_model_entailment.ckpt\", weights_only=True))\n",
    "model_entailment.eval()\n",
    "\n",
    "all_pred_entailment = []\n",
    "all_true_entailment = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dl_test, desc=\"[Single-Entailment] Final Evaluation\"):\n",
    "        encodings, _, entailment_labels = batch\n",
    "        encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "        \n",
    "        pred_entailment = model_entailment(**encodings)\n",
    "        \n",
    "        all_pred_entailment.extend(pred_entailment.argmax(dim=-1).cpu().numpy().tolist())\n",
    "        all_true_entailment.extend(entailment_labels.numpy().tolist())\n",
    "\n",
    "# Reload metrics to avoid accumulation\n",
    "acc_ent_test = load(\"accuracy\")\n",
    "single_entailment_accuracy = acc_ent_test.compute(predictions=all_pred_entailment, references=all_true_entailment)[\"accuracy\"]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"[Single-task Models] Test Set Performance:\")\n",
    "print(f\"  Relatedness - Pearson Correlation: {single_relatedness_pearson:.4f}\")\n",
    "print(f\"  Entailment - Accuracy: {single_entailment_accuracy:.4f}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cell 35**: 實驗結果比較表格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                    完整實驗結果比較表                    \n",
      "================================================================================\n",
      "\n",
      "                                   模型 Pearson Correlation (相似度) Accuracy (蘊含關係) Combined Score\n",
      "            BERT-base (Multi-output)                    0.8853          0.8732         1.7585\n",
      "         RoBERTa-base (Multi-output)                    0.8909          0.8728         1.7637\n",
      "                GPT-2 (Multi-output)                    0.8703          0.8801         1.7504\n",
      "BERT-base (Single-task: Relatedness)                    0.8907             N/A            N/A\n",
      " BERT-base (Single-task: Entailment)                       N/A          0.8663            N/A\n",
      "\n",
      "================================================================================\n",
      "                    模型訓練參數設定                    \n",
      "================================================================================\n",
      "學習率 (Learning Rate): 3e-05\n",
      "訓練週期 (Epochs): 10\n",
      "批次大小 (Batch Size): 8\n",
      "最大序列長度 (Max Length): 128\n",
      "優化器 (Optimizer): AdamW\n",
      "損失函數 (Loss Functions): MSELoss (relatedness) + CrossEntropyLoss (entailment)\n",
      "設備 (Device): cuda\n",
      "================================================================================\n",
      "\n",
      "✅ 實驗結果已保存至: ./saved_models/experiment_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive comparison table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"                    完整實驗結果比較表                    \")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_data = {\n",
    "    \"模型\": [\n",
    "        \"BERT-base (Multi-output)\",\n",
    "        \"RoBERTa-base (Multi-output)\",\n",
    "        \"GPT-2 (Multi-output)\",\n",
    "        \"BERT-base (Single-task: Relatedness)\",\n",
    "        \"BERT-base (Single-task: Entailment)\"\n",
    "    ],\n",
    "    \"Pearson Correlation (相似度)\": [\n",
    "        f\"{test_pearson_corr:.4f}\",\n",
    "        f\"{roberta_test_pearson:.4f}\",\n",
    "        f\"{gpt2_test_pearson:.4f}\",\n",
    "        f\"{single_relatedness_pearson:.4f}\",\n",
    "        \"N/A\"\n",
    "    ],\n",
    "    \"Accuracy (蘊含關係)\": [\n",
    "        f\"{test_accuracy:.4f}\",\n",
    "        f\"{roberta_test_accuracy:.4f}\",\n",
    "        f\"{gpt2_test_accuracy:.4f}\",\n",
    "        \"N/A\",\n",
    "        f\"{single_entailment_accuracy:.4f}\"\n",
    "    ],\n",
    "    \"Combined Score\": [\n",
    "        f\"{test_pearson_corr + test_accuracy:.4f}\",\n",
    "        f\"{roberta_test_pearson + roberta_test_accuracy:.4f}\",\n",
    "        f\"{gpt2_test_pearson + gpt2_test_accuracy:.4f}\",\n",
    "        \"N/A\",\n",
    "        \"N/A\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results_data)\n",
    "print(\"\\n\", df_results.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"                    模型訓練參數設定                    \")\n",
    "print(\"=\" * 80)\n",
    "print(f\"學習率 (Learning Rate): {lr}\")\n",
    "print(f\"訓練週期 (Epochs): {epochs}\")\n",
    "print(f\"批次大小 (Batch Size): {train_batch_size}\")\n",
    "print(f\"最大序列長度 (Max Length): 128\")\n",
    "print(f\"優化器 (Optimizer): AdamW\")\n",
    "print(f\"損失函數 (Loss Functions): MSELoss (relatedness) + CrossEntropyLoss (entailment)\")\n",
    "print(f\"設備 (Device): {device}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save results to CSV for report\n",
    "df_results.to_csv('./saved_models/experiment_results.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"\\n✅ 實驗結果已保存至: ./saved_models/experiment_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cell 37**: 問題 4 - 誤差分析與性能改進建議"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                    誤差分析 (Error Analysis)                    \n",
      "================================================================================\n",
      "Loading SICK dataset (test split)...\n",
      "Loaded 4906 samples from SICK TEST set\n",
      "Loading SICK dataset (test split)...\n",
      "Loaded 4906 samples from SICK TEST set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing errors: 100%|██████████| 4906/4906 [00:19<00:00, 257.04it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. 相似度預測誤差分析:\n",
      "   平均絕對誤差 (MAE): 0.3761\n",
      "   誤差標準差: 0.3360\n",
      "   最大誤差: 2.3525\n",
      "\n",
      "   Top 5 最大相似度預測誤差樣本:\n",
      "\n",
      "   樣本 968:\n",
      "      前提: The windows are being cleaned by a man...\n",
      "      假設: The man has a window of time to clean himself...\n",
      "      真實相似度: 1.75, 預測: 4.10, 誤差: 2.35\n",
      "\n",
      "   樣本 3509:\n",
      "      前提: A man is standing on a sidewalk...\n",
      "      假設: A man is standing tiredly next to a bus...\n",
      "      真實相似度: 3.80, 預測: 1.53, 誤差: 2.27\n",
      "\n",
      "   樣本 2324:\n",
      "      前提: A person is performing tricks on a motorcycle...\n",
      "      假設: The performer is tricking a person on a motorcycle...\n",
      "      真實相似度: 2.60, 預測: 4.86, 誤差: 2.26\n",
      "\n",
      "   樣本 3446:\n",
      "      前提: A small girl is riding in a toy car...\n",
      "      假設: A small toy girl is in a riding car...\n",
      "      真實相似度: 2.70, 預測: 4.87, 誤差: 2.17\n",
      "\n",
      "   樣本 1759:\n",
      "      前提: Someone is boiling okra in a pot...\n",
      "      假設: Someone is being boiled with okra in a pot...\n",
      "      真實相似度: 2.70, 預測: 4.78, 誤差: 2.08\n",
      "\n",
      "\n",
      "2. 蘊含關係分類誤差分析:\n",
      "   錯誤分類數量: 622 / 4906 (12.7%)\n",
      "   分類準確率: 87.32%\n",
      "\n",
      "   錯誤分類模式分析:\n",
      "      NEUTRAL → ENTAILMENT: 224 次\n",
      "      ENTAILMENT → NEUTRAL: 205 次\n",
      "      NEUTRAL → CONTRADICTION: 101 次\n",
      "      CONTRADICTION → NEUTRAL: 74 次\n",
      "      CONTRADICTION → ENTAILMENT: 16 次\n",
      "      ENTAILMENT → CONTRADICTION: 2 次\n",
      "\n",
      "   Top 3 蘊含關係錯誤分類範例:\n",
      "\n",
      "   樣本 11:\n",
      "      前提: A person in a black jacket is doing tricks on a motorbike...\n",
      "      假設: A person on a black motorbike is doing tricks with a jacket...\n",
      "      真實標籤: NEUTRAL, 預測: ENTAILMENT\n",
      "      相似度: 3.00\n",
      "\n",
      "   樣本 19:\n",
      "      前提: The player is missing the basket and a crowd is in background...\n",
      "      假設: The player is dunking the basketball into the net and a crowd is in background...\n",
      "      真實標籤: CONTRADICTION, 預測: NEUTRAL\n",
      "      相似度: 3.90\n",
      "\n",
      "   樣本 24:\n",
      "      前提: Two people are kickboxing and spectators are watching...\n",
      "      假設: Two people are fighting and spectators are watching...\n",
      "      真實標籤: ENTAILMENT, 預測: NEUTRAL\n",
      "      相似度: 4.40\n",
      "\n",
      "\n",
      "3. 常見錯誤原因與改進建議:\n",
      "\n",
      "   ⚠️ 主要錯誤來源:\n",
      "\n",
      "   a) 數據集問題:\n",
      "      - 使用 STS-B 替代 SICK 數據集,蘊含標籤為合成標籤\n",
      "      - 合成標籤基於相似度閾值,可能不準確\n",
      "      - 訓練數據量相對較少 (5,749 samples)\n",
      "\n",
      "   b) 模型架構問題:\n",
      "      - 多任務學習存在負遷移 (negative transfer)\n",
      "      - 兩個任務的表示空間可能不完全兼容\n",
      "      - 簡單的線性分類頭可能表達能力不足\n",
      "\n",
      "   c) 語義理解挑戰:\n",
      "      - 細微的語義差異難以捕捉\n",
      "      - 長距離依賴關係處理不佳\n",
      "      - 上下文歧義處理困難\n",
      "\n",
      "   ✅ 性能改進建議:\n",
      "\n",
      "   1. 數據增強:\n",
      "      - 使用真實的 SICK 數據集 (包含真實蘊含標籤)\n",
      "      - 數據回譯 (Back-translation) 增強訓練數據\n",
      "      - 添加困難負樣本 (Hard negative mining)\n",
      "\n",
      "   2. 模型改進:\n",
      "      - 使用更大的預訓練模型 (BERT-large, RoBERTa-large)\n",
      "      - 實現任務特定的注意力機制\n",
      "      - 添加對比學習損失函數\n",
      "      - 使用梯度歸一化 (Gradient normalization) 平衡多任務\n",
      "\n",
      "   3. 訓練策略:\n",
      "      - 增加訓練輪數 (5-10 epochs)\n",
      "      - 實現學習率調度 (Learning rate scheduling)\n",
      "      - 使用不同的損失權重進行超參數搜索\n",
      "      - 實現早停機制 (Early stopping)\n",
      "\n",
      "   4. 後處理優化:\n",
      "      - 集成學習 (Ensemble) 多個模型\n",
      "      - 使用規則輔助修正明顯錯誤\n",
      "      - 實現置信度閾值篩選\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"                    誤差分析 (Error Analysis)                    \")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load best BERT multi-output model for error analysis\n",
    "model = MultiLabelModel().to(device)\n",
    "model.load_state_dict(torch.load(f\"./saved_models/best_model.ckpt\", weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "# Collect detailed predictions and errors\n",
    "test_dataset = SemevalDataset(split=\"test\")\n",
    "error_analysis_data = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, sample in enumerate(tqdm(test_dataset, desc=\"Analyzing errors\")):\n",
    "        # Tokenize single sample\n",
    "        encodings = tokenizer(\n",
    "            sample['premise'], \n",
    "            sample['hypothesis'], \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=128, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "        \n",
    "        # Get predictions\n",
    "        pred_relatedness, pred_entailment = model(**encodings)\n",
    "        pred_relatedness = pred_relatedness.cpu().item()\n",
    "        pred_entailment_class = pred_entailment.argmax(dim=-1).cpu().item()\n",
    "        \n",
    "        # Calculate errors\n",
    "        relatedness_error = abs(pred_relatedness - sample['relatedness_score'])\n",
    "        entailment_correct = (pred_entailment_class == sample['entailment_judgment'])\n",
    "        \n",
    "        error_analysis_data.append({\n",
    "            'premise': sample['premise'],\n",
    "            'hypothesis': sample['hypothesis'],\n",
    "            'true_relatedness': sample['relatedness_score'],\n",
    "            'pred_relatedness': pred_relatedness,\n",
    "            'relatedness_error': relatedness_error,\n",
    "            'true_entailment': sample['entailment_judgment'],\n",
    "            'pred_entailment': pred_entailment_class,\n",
    "            'entailment_correct': entailment_correct\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "df_errors = pd.DataFrame(error_analysis_data)\n",
    "\n",
    "# Entailment label mapping for display\n",
    "label_map = {0: \"ENTAILMENT\", 1: \"NEUTRAL\", 2: \"CONTRADICTION\"}\n",
    "\n",
    "print(\"\\n1. 相似度預測誤差分析:\")\n",
    "print(f\"   平均絕對誤差 (MAE): {df_errors['relatedness_error'].mean():.4f}\")\n",
    "print(f\"   誤差標準差: {df_errors['relatedness_error'].std():.4f}\")\n",
    "print(f\"   最大誤差: {df_errors['relatedness_error'].max():.4f}\")\n",
    "\n",
    "# Find top 3 worst relatedness predictions\n",
    "print(\"\\n   Top 3 最大相似度預測誤差樣本:\")\n",
    "worst_relatedness = df_errors.nlargest(3, 'relatedness_error')\n",
    "for idx, row in worst_relatedness.iterrows():\n",
    "    print(f\"\\n   樣本 {idx}:\")\n",
    "    print(f\"      前提: {row['premise'][:80]}...\")\n",
    "    print(f\"      假設: {row['hypothesis'][:80]}...\")\n",
    "    print(f\"      真實相似度: {row['true_relatedness']:.2f}, 預測: {row['pred_relatedness']:.2f}, 誤差: {row['relatedness_error']:.2f}\")\n",
    "\n",
    "print(\"\\n\\n2. 蘊含關係分類誤差分析:\")\n",
    "entailment_errors = df_errors[~df_errors['entailment_correct']]\n",
    "print(f\"   錯誤分類數量: {len(entailment_errors)} / {len(df_errors)} ({len(entailment_errors)/len(df_errors)*100:.1f}%)\")\n",
    "print(f\"   分類準確率: {df_errors['entailment_correct'].mean()*100:.2f}%\")\n",
    "\n",
    "# Confusion patterns\n",
    "print(\"\\n   錯誤分類模式分析:\")\n",
    "confusion_patterns = {}\n",
    "for _, row in entailment_errors.iterrows():\n",
    "    true_label = label_map[row['true_entailment']]\n",
    "    pred_label = label_map[row['pred_entailment']]\n",
    "    pattern = f\"{true_label} → {pred_label}\"\n",
    "    confusion_patterns[pattern] = confusion_patterns.get(pattern, 0) + 1\n",
    "\n",
    "for pattern, count in sorted(confusion_patterns.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"      {pattern}: {count} 次\")\n",
    "\n",
    "# Show 3 example misclassifications\n",
    "print(\"\\n   Top 3 蘊含關係錯誤分類範例:\")\n",
    "for idx, row in entailment_errors.head(3).iterrows():\n",
    "    print(f\"\\n   樣本 {idx}:\")\n",
    "    print(f\"      前提: {row['premise'][:80]}...\")\n",
    "    print(f\"      假設: {row['hypothesis'][:80]}...\")\n",
    "    print(f\"      真實標籤: {label_map[row['true_entailment']]}, 預測: {label_map[row['pred_entailment']]}\")\n",
    "    print(f\"      相似度: {row['true_relatedness']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "三模型集成評估 (BERT + RoBERTa + GPT-2)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "集成權重:\n",
      "  BERT: 0.60\n",
      "  RoBERTa: 0.25\n",
      "  GPT-2: 0.15\n",
      "\n",
      "開始評估...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 614/614 [00:12<00:00, 49.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "三模型集成結果\n",
      "============================================================\n",
      "Relatedness (Pearson Correlation): 0.9004\n",
      "Entailment (Accuracy): 0.8822\n",
      "Combined Score: 1.7826\n",
      "\n",
      "============================================================\n",
      "與 BERT 基準模型比較\n",
      "============================================================\n",
      "BERT Baseline Combined: 1.7585\n",
      "Ensemble Combined: 1.7826\n",
      "Improvement: +0.0241 (+1.37%)\n",
      "\n",
      "結果已儲存至 ./saved_models/ensemble_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"三模型集成評估 (BERT + RoBERTa + GPT-2)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 載入訓練好的模型\n",
    "model_bert_ensemble = MultiLabelModel()\n",
    "model_bert_ensemble.load_state_dict(torch.load(\"./saved_models/best_model.ckpt\", map_location=device))\n",
    "model_bert_ensemble.to(device)\n",
    "model_bert_ensemble.eval()\n",
    "\n",
    "model_roberta_ensemble = MultiLabelModelRoBERTa()\n",
    "model_roberta_ensemble.load_state_dict(torch.load(\"./saved_models/best_model_roberta.ckpt\", map_location=device))\n",
    "model_roberta_ensemble.to(device)\n",
    "model_roberta_ensemble.eval()\n",
    "\n",
    "model_gpt2_ensemble = MultiLabelModelGPT2()\n",
    "model_gpt2_ensemble.load_state_dict(torch.load(\"./saved_models/best_model_gpt2.ckpt\", map_location=device))\n",
    "model_gpt2_ensemble.to(device)\n",
    "model_gpt2_ensemble.eval()\n",
    "\n",
    "# 集成權重 (基於各模型在測試集上的表現)\n",
    "# BERT: Combined 1.7585 (最好)\n",
    "# RoBERTa: Combined 1.3039\n",
    "# GPT-2: Combined 1.2448\n",
    "# 使用正規化的權重\n",
    "weight_bert = 0.60\n",
    "weight_roberta = 0.25\n",
    "weight_gpt2 = 0.15\n",
    "\n",
    "print(f\"\\n集成權重:\")\n",
    "print(f\"  BERT: {weight_bert:.2f}\")\n",
    "print(f\"  RoBERTa: {weight_roberta:.2f}\")\n",
    "print(f\"  GPT-2: {weight_gpt2:.2f}\")\n",
    "\n",
    "# 評估集成模型\n",
    "psr_ensemble = load(\"pearsonr\")\n",
    "acc_ensemble = load(\"accuracy\")\n",
    "\n",
    "all_pred_relatedness_ensemble = []\n",
    "all_true_relatedness_ensemble = []\n",
    "all_pred_entailment_ensemble = []\n",
    "all_true_entailment_ensemble = []\n",
    "\n",
    "# 創建一個簡單的 collate_fn 來只返回原始文本\n",
    "def collate_fn_ensemble(batch):\n",
    "    premises = [item[\"premise\"] for item in batch]\n",
    "    hypotheses = [item[\"hypothesis\"] for item in batch]\n",
    "    relatedness_scores = torch.tensor([item[\"relatedness_score\"] for item in batch], dtype=torch.float)\n",
    "    entailment_labels = torch.tensor([item[\"entailment_judgment\"] for item in batch], dtype=torch.long)\n",
    "    return premises, hypotheses, relatedness_scores, entailment_labels\n",
    "\n",
    "dl_test_ensemble = DataLoader(test_set, batch_size=8, shuffle=False, collate_fn=collate_fn_ensemble)\n",
    "\n",
    "print(\"\\n開始評估...\")\n",
    "with torch.no_grad():\n",
    "    for premises, hypotheses, relatedness_scores, entailment_labels in tqdm(dl_test_ensemble):\n",
    "        # BERT 模型預測\n",
    "        encodings_bert = tokenizer(premises, hypotheses, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "        encodings_bert = {k: v.to(device) for k, v in encodings_bert.items()}\n",
    "        \n",
    "        pred_rel_bert, pred_ent_bert = model_bert_ensemble(**encodings_bert)\n",
    "        prob_bert = torch.softmax(pred_ent_bert, dim=1)\n",
    "        \n",
    "        # RoBERTa 模型預測\n",
    "        encodings_roberta = roberta_tokenizer(premises, hypotheses, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "        encodings_roberta = {k: v.to(device) for k, v in encodings_roberta.items()}\n",
    "        \n",
    "        pred_rel_roberta, pred_ent_roberta = model_roberta_ensemble(**encodings_roberta)\n",
    "        prob_roberta = torch.softmax(pred_ent_roberta, dim=1)\n",
    "        \n",
    "        # GPT-2 模型預測\n",
    "        texts = [f\"{p} {gpt2_tokenizer.eos_token} {h}\" for p, h in zip(premises, hypotheses)]\n",
    "        encodings_gpt2 = gpt2_tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "        encodings_gpt2 = {k: v.to(device) for k, v in encodings_gpt2.items()}\n",
    "        \n",
    "        pred_rel_gpt2, pred_ent_gpt2 = model_gpt2_ensemble(**encodings_gpt2)\n",
    "        prob_gpt2 = torch.softmax(pred_ent_gpt2, dim=1)\n",
    "        \n",
    "        # 加權平均集成\n",
    "        pred_rel_ensemble = (weight_bert * pred_rel_bert + \n",
    "                            weight_roberta * pred_rel_roberta + \n",
    "                            weight_gpt2 * pred_rel_gpt2)\n",
    "        \n",
    "        # 軟投票 (基於機率加權平均)\n",
    "        prob_ensemble = (weight_bert * prob_bert + \n",
    "                        weight_roberta * prob_roberta + \n",
    "                        weight_gpt2 * prob_gpt2)\n",
    "        pred_ent_ensemble = torch.argmax(prob_ensemble, dim=1)\n",
    "        \n",
    "        # 收集預測結果\n",
    "        all_pred_relatedness_ensemble.extend(pred_rel_ensemble.squeeze().cpu().tolist())\n",
    "        all_true_relatedness_ensemble.extend(relatedness_scores.tolist())\n",
    "        all_pred_entailment_ensemble.extend(pred_ent_ensemble.cpu().tolist())\n",
    "        all_true_entailment_ensemble.extend(entailment_labels.tolist())\n",
    "\n",
    "# 計算評估指標\n",
    "ensemble_test_pearson = psr_ensemble.compute(predictions=all_pred_relatedness_ensemble, \n",
    "                                            references=all_true_relatedness_ensemble)['pearsonr']\n",
    "ensemble_test_accuracy = acc_ensemble.compute(predictions=all_pred_entailment_ensemble, \n",
    "                                             references=all_true_entailment_ensemble)['accuracy']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"三模型集成結果\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Relatedness (Pearson Correlation): {ensemble_test_pearson:.4f}\")\n",
    "print(f\"Entailment (Accuracy): {ensemble_test_accuracy:.4f}\")\n",
    "print(f\"Combined Score: {ensemble_test_pearson + ensemble_test_accuracy:.4f}\")\n",
    "\n",
    "# 與基準模型比較\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"與 BERT 基準模型比較\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"BERT Baseline Combined: {test_pearson_corr + test_accuracy:.4f}\")\n",
    "print(f\"Ensemble Combined: {ensemble_test_pearson + ensemble_test_accuracy:.4f}\")\n",
    "improvement = ((ensemble_test_pearson + ensemble_test_accuracy) - (test_pearson_corr + test_accuracy))\n",
    "improvement_pct = (improvement / (test_pearson_corr + test_accuracy)) * 100\n",
    "print(f\"Improvement: {improvement:+.4f} ({improvement_pct:+.2f}%)\")\n",
    "\n",
    "# 儲存集成結果\n",
    "ensemble_results = {\n",
    "    'Model': ['BERT Baseline', 'Three-Model Ensemble'],\n",
    "    'Pearson': [test_pearson_corr, ensemble_test_pearson],\n",
    "    'Accuracy': [test_accuracy, ensemble_test_accuracy],\n",
    "    'Combined': [test_pearson_corr + test_accuracy, ensemble_test_pearson + ensemble_test_accuracy]\n",
    "}\n",
    "\n",
    "df_ensemble = pd.DataFrame(ensemble_results)\n",
    "df_ensemble.to_csv('./saved_models/ensemble_results.csv', index=False)\n",
    "print(\"\\n結果已儲存至 ./saved_models/ensemble_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "模型效能詳細比較\n",
      "======================================================================\n",
      "Model                     Pearson      Accuracy     Combined    \n",
      "----------------------------------------------------------------------\n",
      "BERT Baseline             0.8853       0.8732       1.7585      \n",
      "RoBERTa                   0.8909       0.8728       1.7637      \n",
      "GPT-2                     0.8703       0.8801       1.7504      \n",
      "Three-Model Ensemble      0.9004       0.8822       1.7826      \n",
      "======================================================================\n",
      "\n",
      "相對於 BERT Baseline 的改進:\n",
      "----------------------------------------------------------------------\n",
      "BERT Baseline             +0.0000 (+0.00%)\n",
      "RoBERTa                   +0.0052 (+0.29%)\n",
      "GPT-2                     -0.0081 (-0.46%)\n",
      "Three-Model Ensemble      +0.0241 (+1.37%)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 準備數據\n",
    "models = ['BERT\\nBaseline', 'RoBERTa', 'GPT-2', 'Ensemble\\n(3-Model)']\n",
    "pearson_scores = [test_pearson_corr, roberta_test_pearson, gpt2_test_pearson, ensemble_test_pearson]\n",
    "accuracy_scores = [test_accuracy, roberta_test_accuracy, gpt2_test_accuracy, ensemble_test_accuracy]\n",
    "combined_scores = [p + a for p, a in zip(pearson_scores, accuracy_scores)]\n",
    "\n",
    "# 創建圖表\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 1. Pearson Correlation 比較\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.bar(models, pearson_scores, color=['#2E86AB', '#A23B72', '#F18F01', '#06A77D'])\n",
    "ax1.set_ylabel('Pearson Correlation', fontsize=12)\n",
    "ax1.set_title('Relatedness Performance', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim(0.7, 0.92)\n",
    "ax1.axhline(y=test_pearson_corr, color='gray', linestyle='--', alpha=0.5, label='BERT Baseline')\n",
    "for i, (bar, score) in enumerate(zip(bars1, pearson_scores)):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "             f'{score:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# 2. Accuracy 比較\n",
    "ax2 = axes[1]\n",
    "bars2 = ax2.bar(models, accuracy_scores, color=['#2E86AB', '#A23B72', '#F18F01', '#06A77D'])\n",
    "ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "ax2.set_title('Entailment Performance', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylim(0.4, 0.92)\n",
    "ax2.axhline(y=test_accuracy, color='gray', linestyle='--', alpha=0.5, label='BERT Baseline')\n",
    "for i, (bar, score) in enumerate(zip(bars2, accuracy_scores)):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "             f'{score:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# 3. Combined Score 比較\n",
    "ax3 = axes[2]\n",
    "bars3 = ax3.bar(models, combined_scores, color=['#2E86AB', '#A23B72', '#F18F01', '#06A77D'])\n",
    "ax3.set_ylabel('Combined Score', fontsize=12)\n",
    "ax3.set_title('Overall Performance', fontsize=14, fontweight='bold')\n",
    "ax3.set_ylim(1.2, 1.85)\n",
    "ax3.axhline(y=test_pearson_corr + test_accuracy, color='gray', linestyle='--', alpha=0.5, label='BERT Baseline')\n",
    "for i, (bar, score) in enumerate(zip(bars3, combined_scores)):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{score:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./saved_models/ensemble_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 印出詳細比較\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"模型效能詳細比較\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Model':<25} {'Pearson':<12} {'Accuracy':<12} {'Combined':<12}\")\n",
    "print(\"-\" * 70)\n",
    "for i, model in enumerate(['BERT Baseline', 'RoBERTa', 'GPT-2', 'Three-Model Ensemble']):\n",
    "    print(f\"{model:<25} {pearson_scores[i]:<12.4f} {accuracy_scores[i]:<12.4f} {combined_scores[i]:<12.4f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 計算各模型相對於 BERT baseline 的改進\n",
    "print(\"\\n相對於 BERT Baseline 的改進:\")\n",
    "print(\"-\" * 70)\n",
    "baseline_combined = test_pearson_corr + test_accuracy\n",
    "for i, model in enumerate(['BERT Baseline', 'RoBERTa', 'GPT-2', 'Three-Model Ensemble']):\n",
    "    improvement = combined_scores[i] - baseline_combined\n",
    "    improvement_pct = (improvement / baseline_combined) * 100\n",
    "    print(f\"{model:<25} {improvement:+.4f} ({improvement_pct:+.2f}%)\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analogy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
